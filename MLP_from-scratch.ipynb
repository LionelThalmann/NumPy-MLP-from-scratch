{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299c7ad",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552c2e9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ▸▸ Amazon Reviews – download via Kaggle API if not present ▸▸\n",
    "csv_path = Path('datasets/reviews/amazon_review_ID.shuf.lrn.csv')\n",
    "if not csv_path.exists():\n",
    "    !kaggle competitions download -c 184-702-tu-ml-2025-s-reviews -p datasets/reviews --force\n",
    "    !unzip -o datasets/reviews/184-702-tu-ml-2025-s-reviews.zip -d datasets/reviews\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9311af",
   "metadata": {},
   "source": [
    "# Dataset Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88eba3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X_all = df.drop(columns=['ID', 'Class']).values         \n",
    "y_all = df['Class'].values                              \n",
    "\n",
    "# schuffle\n",
    "perm   = np.random.permutation(len(df))\n",
    "X_all  = X_all[perm]            \n",
    "y_all  = y_all[perm]\n",
    "\n",
    "# train/ test split (70/30)\n",
    "split  = int(0.7 * X_all.shape[0])\n",
    "X_train, X_test = X_all[:split, :],  X_all[split:, :]\n",
    "y_train_string, y_test_string = y_all[:split],     y_all[split:]\n",
    "n1, m1 = X_train.shape\n",
    "print(n1,m1)\n",
    "\n",
    "\n",
    "# Lable encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_string)  \n",
    "y_test  = le.transform(y_test_string) \n",
    "\n",
    "\n",
    "\n",
    "print(f\"X_train = {X_train.shape}, X_test = {X_test.shape} y_train = {y_train.shape}, y_test {y_test.shape} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f9dc",
   "metadata": {},
   "source": [
    "# Dataset Credit Card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b299e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "df_c = fetch_ucirepo(id=350) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba1865",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X_credit  = df_c.data.features.copy()\n",
    "y_credit  = df_c.data.targets.iloc[:, 0]         \n",
    "\n",
    "print(repr(X_credit.columns.tolist()))\n",
    "\n",
    "# remove leading / trailing blanks inall column labels\n",
    "X_credit.columns = X_credit.columns.str.strip()\n",
    "\n",
    "X_credit = X_credit.drop(columns='ID', errors='ignore')\n",
    "\n",
    "# In the raw data there are some values which diont fit to a category so we collapse them into \"other\" bucket\n",
    "X_credit['X3'] = X_credit['X3'].replace({0: 4, 5: 4, 6: 4})\n",
    "X_credit['X4']  = X_credit['X4'].replace({0: 3})\n",
    "\n",
    "# One-hot encode the three categorical columns\n",
    "cat_cols = ['X2', 'X3', 'X4']   \n",
    "X_cat = pd.get_dummies(X_credit[cat_cols], drop_first=True)     \n",
    "\n",
    "# numeric + ordinal columns (everything that is NOT categorical)\n",
    "num_cols = X_credit.columns.difference(cat_cols)\n",
    "X_num = X_credit[num_cols].astype(float)\n",
    "\n",
    "# stitch everything together\n",
    "X_pre = pd.concat([X_cat, X_num], axis=1)\n",
    "\n",
    "# Convert to NumPy, shuffle, 70 / 30 split\n",
    "X_all_c = X_pre.to_numpy(dtype=np.float32)\n",
    "y_all_c = y_credit.to_numpy(dtype=np.int64)\n",
    "\n",
    "perm = np.random.permutation(len(X_all_c))\n",
    "X_all_c, y_all_c = X_all_c[perm], y_all_c[perm]\n",
    "\n",
    "split = int(0.7 * len(X_all))\n",
    "X_train_c, X_test_c = X_all_c[:split],  X_all_c[split:]\n",
    "y_train_c, y_test_c = y_all_c[:split],  y_all_c[split:]\n",
    "\n",
    "print(\"train shape:\", X_train_c.shape, \"test shape:\", X_test_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a90abb",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d35bb6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def he_init(fan_in: int, fan_out: int):\n",
    "    std_dev = np.sqrt(2.0 / fan_in)\n",
    "    return np.random.randn(fan_in, fan_out) * std_dev\n",
    "\n",
    "\n",
    "def params(n_x, hidden_sizes, n_y):\n",
    "    if isinstance(hidden_sizes, int):\n",
    "        hidden_sizes = [hidden_sizes]\n",
    "\n",
    "    layer_sizes = [n_x] + list(hidden_sizes) + [n_y]\n",
    "\n",
    "    weights = []  \n",
    "    biases  = []  \n",
    "\n",
    "    for fan_in, fan_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        weights.append(he_init(fan_in, fan_out))\n",
    "        biases.append(np.zeros((1, fan_out)))\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def activation(Z, kind):\n",
    "    if kind == \"relu\":\n",
    "        return np.maximum(0, Z)\n",
    "    if kind == \"sigmoid\":\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "    if kind == \"tanh\":\n",
    "        return np.tanh(Z)\n",
    "    raise ValueError(f\"Unknown activation: {kind}\")\n",
    "\n",
    "\n",
    "def deriv_activation(Z, A, kind):\n",
    "    if kind == \"relu\":\n",
    "        return (Z > 0).astype(Z.dtype)\n",
    "    if kind == \"sigmoid\":\n",
    "        return A * (1.0 - A)\n",
    "    if kind == \"tanh\":\n",
    "        return 1.0 - np.power(A, 2)\n",
    "    raise ValueError(f\"Unknown activation: {kind}\")\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    shiftZ = Z - np.max(Z, axis=1, keepdims=True)  # subtract max per row\n",
    "    expZ   = np.exp(shiftZ)\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    out = np.zeros((y.size, n_classes))\n",
    "    out[np.arange(y.size), y] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def forward_prop(Ws, bs, X, act):\n",
    "    \"\"\"\n",
    "        Zs : list of pre‑activations for hidden layers\n",
    "        As : list of post‑activations for hidden layers  (same length as Zs)\n",
    "        ZL : logits of the output layer (pre‑softmax)\n",
    "        AL : probabilities after softmax\n",
    "    \"\"\"\n",
    "    Zs = []             \n",
    "    As = [X]             \n",
    "\n",
    "    for W, b in zip(Ws[:-1], bs[:-1]):\n",
    "        Z = As[-1] @ W + b            # matrix multiply + bias\n",
    "        A = activation(Z, act)        \n",
    "        Zs.append(Z)\n",
    "        As.append(A)\n",
    "\n",
    "    # Output layer\n",
    "    ZL = As[-1] @ Ws[-1] + bs[-1]\n",
    "    AL = softmax(ZL)\n",
    "\n",
    "    return Zs, As[1:], ZL, AL        \n",
    "\n",
    "\n",
    "\n",
    "def backward_prop(Zs, As, ZL, AL, Ws, X, y, act):\n",
    "   \n",
    "    m = X.shape[0]\n",
    "    y_onehot = one_hot(y)\n",
    "\n",
    "    # Gradient at the output layer \n",
    "    dZ = AL - y_onehot                # (m, n_classes)\n",
    "\n",
    "    dWs = []\n",
    "    dbs = []\n",
    "\n",
    "    total_layers = len(Ws)\n",
    "    # We iterate backwards (L‑1, L‑2, ..., 0)\n",
    "    for layer_idx in reversed(range(total_layers)):\n",
    "        A_prev = As[layer_idx - 1] if layer_idx > 0 else X\n",
    "\n",
    "        dW = (A_prev.T @ dZ) / m      # weight gradient\n",
    "        db = np.mean(dZ, axis=0, keepdims=True)  # bias gradient\n",
    "\n",
    "        # We Store gradients at the front so order matches Ws later on\n",
    "        dWs.insert(0, dW)\n",
    "        dbs.insert(0, db)\n",
    "\n",
    "        # Move gradient one layer backward, unless we just finished first layer\n",
    "        if layer_idx > 0:\n",
    "            dA_prev = dZ @ Ws[layer_idx].T\n",
    "            dZ = dA_prev * deriv_activation(Zs[layer_idx - 1], As[layer_idx - 1], act)\n",
    "\n",
    "    return dWs, dbs\n",
    "\n",
    "\n",
    "def update_params(Ws, bs, dWs, dbs, alpha: float = 0.01):\n",
    "    for i in range(len(Ws)):\n",
    "        Ws[i] -= alpha * dWs[i]\n",
    "        bs[i] -= alpha * dbs[i]\n",
    "    return Ws, bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757df23",
   "metadata": {},
   "source": [
    "# Fit and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da965b5c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(AL, y):\n",
    "    m = y.shape[0]\n",
    "    y_oh = one_hot(y)\n",
    "    return -np.sum(y_oh * np.log(AL + 1e-15)) / m  # 1e‑15 avoids log(0)\n",
    "\n",
    "\n",
    "def predict(Ws, bs, X, act):\n",
    "    _, _, _, AL = forward_prop(Ws, bs, X, act)\n",
    "    return np.argmax(AL, axis=1)\n",
    "\n",
    "\n",
    "def fit(X, y, *, n_h, n_layers, iters, alpha, act, es_tol=1e-4, es_patience=10):\n",
    "    n_x = X.shape[1]\n",
    "    n_y = y.max() + 1\n",
    "\n",
    "    hidden_sizes = [n_h] * n_layers if n_layers > 1 else n_h\n",
    "\n",
    "    Ws, bs = params(n_x, hidden_sizes, n_y)\n",
    "    \n",
    "    # Early Stopping\n",
    "    best_loss = np.inf\n",
    "    stale     = 0\n",
    "\n",
    "    for i in range(iters):\n",
    "        Zs, As, ZL, AL = forward_prop(Ws, bs, X, act)\n",
    "        loss = compute_loss(AL, y)\n",
    "        dWs, dbs = backward_prop(Zs, As, ZL, AL, Ws, X, y, act)\n",
    "        Ws, bs = update_params(Ws, bs, dWs, dbs, alpha)\n",
    "        \n",
    "        \n",
    "        if loss < best_loss - es_tol:\n",
    "            best_loss = loss\n",
    "            stale     = 0\n",
    "        else:\n",
    "            stale    += 1\n",
    "            if stale >= es_patience:\n",
    "                if i % 100 != 0:       \n",
    "                    print(f\"iter {i:5d}: loss {loss:.4f}  (early stop)\")\n",
    "                break\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"iter {i:5d}: loss {loss:.4f}\")\n",
    "\n",
    "    \n",
    "    return Ws, bs\n",
    "\n",
    "def inspect_model(weights, biases, dtype=np.float64):\n",
    "    n_params = sum(w.size for w in weights) + sum(b.size for b in biases)\n",
    "    ram = n_params * np.dtype(dtype).itemsize\n",
    "    return n_params, ram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45129d",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6adb04",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np          # ← NEW\n",
    "\n",
    "def grid_search(X_train, y_train,\n",
    "                X_val, y_val,\n",
    "                param_grid,\n",
    "                scaler,\n",
    "                n_samples=None,      \n",
    "                verbose=True):\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    if n_samples is not None and n_samples < len(combos):\n",
    "        rng = np.random.default_rng(42)          \n",
    "        combos = rng.choice(combos, size=n_samples, replace=False)\n",
    "        combos = list(combos)                    \n",
    "    \n",
    "    best_score   = -np.inf\n",
    "    best_params  = None\n",
    "    best_weights = None\n",
    "\n",
    "    for i, params in enumerate(combos, start=1):\n",
    "        fit_kwargs = params.copy()\n",
    "\n",
    "        X_tr = scaler.transform(X_train) if scaler is not None else X_train\n",
    "        Ws, bs = fit(X_tr, y_train, **fit_kwargs)\n",
    "\n",
    "        X_v = scaler.transform(X_val) if scaler is not None else X_val\n",
    "        y_pred = predict(Ws, bs, X_v, act=fit_kwargs.get(\"act\", \"relu\"))\n",
    "        score = np.mean(y_pred == y_val)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{i:03d}/{len(combos)}] {params} → acc={score:.3f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score, best_params, best_weights = score, params, (Ws, bs)\n",
    "\n",
    "    return best_params, best_weights, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc6ec6",
   "metadata": {},
   "source": [
    "# Evalutaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17234ded",
   "metadata": {},
   "source": [
    "## 1. Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943c41",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_h\":     [10, 64, 500],\n",
    "    \"n_layers\": [1, 2, 3],\n",
    "    \"alpha\":   [0.1, 0.05, 0.01],\n",
    "    \"iters\":   [50, 100, 500],\n",
    "    \"act\":     [\"relu\", \"tanh\", \"sigmoid\"],\n",
    "}\n",
    "\n",
    "best_params, best_weights, best_val_acc = grid_search(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    param_grid,\n",
    "    scaler=scaler,\n",
    "    n_samples=40\n",
    "    )     \n",
    "\n",
    "print(\"\\nBest combo:\", best_params)\n",
    "print(\"Validation accuracy:\", best_val_acc)\n",
    "\n",
    "n_params, ram_bytes = inspect_model(*best_weights, dtype=np.float64)\n",
    "print(f\"Learnable parameters: {n_params:,}\")\n",
    "print(f\"Estimated RAM for weights+biases: {ram_bytes/1024:.1f} KiB\")\n",
    "\n",
    "X_test_s = scaler.transform(X_test)\n",
    "y_pred   = predict(*best_weights, X_test_s, act=best_params[\"act\"])\n",
    "test_acc = np.mean(y_pred == y_test)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63312f",
   "metadata": {},
   "source": [
    "## 2. Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ddce6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "scaler_c = StandardScaler().fit(X_train_c)\n",
    "\n",
    "\n",
    "param_grid_c = {\n",
    "    \"n_h\":      [10, 64, 500],\n",
    "    \"n_layers\": [1, 2, 3],\n",
    "    \"alpha\":    [0.1, 0.05, 0.01],\n",
    "    \"iters\":    [50, 100, 500],\n",
    "    \"act\":      [\"relu\", \"tanh\", \"sigmoid\"],\n",
    "}\n",
    "\n",
    "best_params_c, best_weights_c, best_val_acc_c = grid_search(\n",
    "    X_train_c, y_train_c,\n",
    "    X_test_c,  y_test_c,          \n",
    "    param_grid_c,\n",
    "    scaler=scaler_c, \n",
    "    n_samples=40             \n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nBest combo (_c):\", best_params_c)\n",
    "print(\"Validation accuracy (_c):\", best_val_acc_c)\n",
    "\n",
    "\n",
    "n_params_c, ram_bytes_c = inspect_model(*best_weights_c, dtype=np.float64)\n",
    "print(f\"Learnable parameters: {n_params_c:,}\")\n",
    "print(f\"Estimated RAM for weights+biases: {ram_bytes_c/1024:.1f} KiB\")\n",
    "\n",
    "X_test_s_c = scaler_c.transform(X_test_c)         \n",
    "y_pred_c   = predict(*best_weights_c, X_test_s_c, act=best_params_c[\"act\"])\n",
    "test_acc_c = np.mean(y_pred_c == y_test_c)\n",
    "print(f\"Test accuracy (_c): {test_acc_c:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "octave",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
